# .env file

# --- AI Model Configuration ---
# This is the single variable to control your reasoning model.
# Options: phi3, tinyllama, deepseek-coder:1.3b
OLLAMA_LLM_MODEL=phi3

# --- Service Configuration ---
OLLAMA_EMBED_MODEL=nomic-embed-text
QDRANT_COLLECTION=isc2_toronto_v3

# --- AI Prompt Configuration ---
LLM_SYSTEM_PROMPT="You are a helpful assistant for the (ISC)Â² Toronto Chapter. Your tone should be friendly and concise. Base your answer ONLY on the information within the provided CONTEXT. If the CONTEXT does not contain the information to answer the QUESTION, you must say 'I do not have enough information to answer this question.'"
